{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C1UFxvFAuCmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685c6ba4-aaa7-4a81-fd24-49753350835f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install scipy opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from kagglehub import dataset_download\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "piKnMg4yyTwf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"agungpambudi/mnist-multiple-dataset-comprehensive-analysis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTdD6x3RzIeX",
        "outputId": "16ec2445-ebe1-4765-c44c-4ba0b8db63a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/mnist-multiple-dataset-comprehensive-analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/mnist-multiple-dataset-comprehensive-analysis\"\n",
        "base_path = os.path.join(dataset_path, \"PolyMNIST\", \"MMNIST\")\n",
        "\n",
        "data = []\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    split_path = os.path.join(base_path, split)\n",
        "    if not os.path.exists(split_path):\n",
        "        continue\n",
        "    for modality in os.listdir(split_path):\n",
        "        modality_path = os.path.join(split_path, modality)\n",
        "        if not os.path.isdir(modality_path):\n",
        "            continue\n",
        "        for file in os.listdir(modality_path):\n",
        "            if file.endswith(\".png\"):\n",
        "                try:\n",
        "                    label = int(file.split(\".\")[1])  # Ej: 1234.5.png → 5\n",
        "                    full_path = os.path.join(modality_path, file)\n",
        "                    data.append({\n",
        "                        \"file_path\": full_path,\n",
        "                        \"label\": label,\n",
        "                        \"modality\": modality,\n",
        "                        \"split\": split\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error con archivo: {file} → {e}\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"DataFrame generado con\", len(df), \"registros.\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "ZbpmXrD1x7aW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a289a9-6490-49aa-f444-6aba2f6573fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame generado con 350000 registros.\n",
            "                                           file_path  label modality  split\n",
            "0  /kaggle/input/mnist-multiple-dataset-comprehen...      9       m4  train\n",
            "1  /kaggle/input/mnist-multiple-dataset-comprehen...      0       m4  train\n",
            "2  /kaggle/input/mnist-multiple-dataset-comprehen...      0       m4  train\n",
            "3  /kaggle/input/mnist-multiple-dataset-comprehen...      9       m4  train\n",
            "4  /kaggle/input/mnist-multiple-dataset-comprehen...      9       m4  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ElasticTransform:\n",
        "    def __init__(self, alpha=36, sigma=6):\n",
        "        self.alpha = alpha\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, img: Image.Image) -> Image.Image:\n",
        "        arr = np.array(img)\n",
        "        shape = arr.shape[:2]\n",
        "        # campos de desplazamiento\n",
        "        dx = (np.random.rand(*shape)*2 - 1)\n",
        "        dy = (np.random.rand(*shape)*2 - 1)\n",
        "        dx = gaussian_filter(dx, self.sigma) * self.alpha\n",
        "        dy = gaussian_filter(dy, self.sigma) * self.alpha\n",
        "        # mallas\n",
        "        x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
        "        map_x = (x + dx).astype(np.float32)\n",
        "        map_y = (y + dy).astype(np.float32)\n",
        "        # aplica remap canal por canal\n",
        "        warped = np.stack([\n",
        "            cv2.remap(arr[...,c], map_x, map_y,\n",
        "                      interpolation=cv2.INTER_LINEAR,\n",
        "                      borderMode=cv2.BORDER_REFLECT_101)\n",
        "            for c in range(3)\n",
        "        ], axis=2)\n",
        "        return Image.fromarray(warped)\n",
        "\n",
        "# Transforms para train / test\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    ElasticTransform(alpha=36, sigma=6),\n",
        "    transforms.ToTensor(),                                # [0,1]\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))     # [-1,1]\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "wL1-DRsjxZRF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RGBMNISTDataset(Dataset):\n",
        "    def __init__(self, df, split, transform):\n",
        "        self.sub = df[df['split']==split].reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sub)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        row = self.sub.loc[i]\n",
        "        img = Image.open(row.file_path).convert(\"RGB\")\n",
        "        return self.transform(img), row.label\n",
        "\n",
        "train_ds = RGBMNISTDataset(df, 'train', transform_train)\n",
        "test_ds  = RGBMNISTDataset(df, 'test',  transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=8)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "id": "QJ_0aSxJxm0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f567024b-b6ad-44fc-db48-1e05d21bbdb3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet(nn.Module):\n",
        "    def __init__(self, input_dim=3*28*28, hidden=800, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B,3,28,28]\n",
        "        x = x.view(x.size(0), -1)    # aplanar → [B,3*28*28]\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iC0bZdRJxqJ2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TwoLayerNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds==y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return 100*correct/total\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "epochs = 5\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out  = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "\n",
        "    train_acc = evaluate(train_loader)\n",
        "    test_acc  = evaluate(test_loader)\n",
        "    avg_loss  = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} | \"\n",
        "          f\"Loss: {avg_loss:.4f} | \"\n",
        "          f\"Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "_PbPBz8pxsPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a0fc9c-9736-4a07-ef70-12bc74202fbb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Loss: 0.9284 | Train Acc: 77.19% | Test Acc: 85.29%\n",
            "Epoch 2/5 | Loss: 0.7175 | Train Acc: 80.32% | Test Acc: 87.60%\n",
            "Epoch 3/5 | Loss: 0.6560 | Train Acc: 81.60% | Test Acc: 88.28%\n",
            "Epoch 4/5 | Loss: 0.6225 | Train Acc: 82.70% | Test Acc: 89.22%\n",
            "Epoch 5/5 | Loss: 0.6000 | Train Acc: 81.12% | Test Acc: 87.69%\n"
          ]
        }
      ]
    }
  ]
}